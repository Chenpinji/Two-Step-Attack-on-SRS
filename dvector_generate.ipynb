{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d145d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import scipy.io.wavfile as wf\n",
    "import numpy as np\n",
    "import base64\n",
    "import hashlib\n",
    "import json \n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "import scipy\n",
    "import random\n",
    "#wav2mel = torch.jit.load(\"wav2mel.pt\")\n",
    "dvector = torch.jit.load(\"dvector-step250000.pt\").eval()\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "\n",
    "hop_length = 128\n",
    "n_mels = 40\n",
    "sample_rate = 16000\n",
    "\n",
    "def dvector_extraction(wav_tensor1):\n",
    "    #transform = torchaudio.transforms.MFCC(sample_rate = sample_rate,n_mfcc = 40, melkwargs={\"n_fft\": 1024, \"hop_length\": 128, \"n_mels\": 64, \"center\": False},)\n",
    "    transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=False,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=1,\n",
    "    norm='slaney',\n",
    "    onesided=True,\n",
    "    n_mels=n_mels,)\n",
    "    mel_tensor1 = transform(wav_tensor1).T\n",
    "    emb_tensor1 = dvector.embed_utterance(mel_tensor1)\n",
    "    return emb_tensor1\n",
    "    \n",
    "def loss_function(AdvTensor, TarTensor):\n",
    "    return torch.norm(AdvTensor - TarTensor)\n",
    "def getSign(timestamp, nonce):\n",
    "    hs = hashlib.sha256()\n",
    "    appkey = \"zvpcvm5hxib3jz3vt2jshqxjndcywwo2qnx7s6iy\"\n",
    "    secret = '3c8a61e2bdb0ee2d0af74814142ba2ee'\n",
    "    hs.update((appkey + timestamp + secret + nonce).encode('utf-8'))\n",
    "    signature = hs.hexdigest().upper()\n",
    "    return signature\n",
    "def identifyFeatureByGroupId(confirmFeatureFileName):\n",
    "    identify_feature = open(confirmFeatureFileName, 'rb').read()\n",
    "    # 声纹base64字符串\n",
    "    audio_data = base64.b64encode(identify_feature)\n",
    "    timestamp = str(int(time.time() * 1000))\n",
    "    nonce = str(uuid.uuid1()).replace('-', '')\n",
    "    sign = getSign(timestamp, nonce)\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    appkey = \"zvpcvm5hxib3jz3vt2jshqxjndcywwo2qnx7s6iy\"\n",
    "    groupId = '15'\n",
    "    host = 'https://ai-vpr.hivoice.cn'\n",
    "    identifyFeatureByGroupIdEndPoint = '/vpr/v1/identifyFeatureByGroupId'\n",
    "    identify_feature_param = {\n",
    "        \"appkey\": appkey,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"nonce\": nonce,\n",
    "        \"sign\": sign,\n",
    "        \"groupId\": groupId,\n",
    "        \"topN\": 10,\n",
    "        \"audioData\": audio_data.decode(),\n",
    "        \"audioSampleRate\": 16000,\n",
    "        \"audioFormat\": \"wav\"\n",
    "    }\n",
    "    #print('identify_feature_param', identify_feature_param)\n",
    "    identify_feature_resp = requests.post(url=host + identifyFeatureByGroupIdEndPoint,\n",
    "                                          data=json.dumps(identify_feature_param),\n",
    "                                          headers=headers)\n",
    "    identify_feature_result = json.loads(identify_feature_resp.content)\n",
    "    return identify_feature_result['data'][0]['featureInfo'], identify_feature_result['data'][0]['score'],identify_feature_result['data'][1]['featureInfo'], identify_feature_result['data'][1]['score']\n",
    "# for param in wav2mel.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in dvector.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7812454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 loss=8.67409896850586\n",
      "epoch=100 loss=3.6485090255737305\n",
      "epoch=200 loss=3.2117512226104736\n",
      "epoch=300 loss=2.983950138092041\n",
      "epoch=400 loss=2.844736337661743\n",
      "epoch=500 loss=2.729119062423706\n",
      "epoch=600 loss=2.6353256702423096\n",
      "epoch=672 loss=2.554264783859253\n",
      "0\n",
      "epoch=0 loss=10.836172103881836\n",
      "epoch=100 loss=7.329015254974365\n",
      "epoch=200 loss=6.672706127166748\n",
      "epoch=300 loss=6.089045524597168\n",
      "epoch=400 loss=5.437991619110107\n",
      "epoch=500 loss=5.21044921875\n",
      "epoch=600 loss=5.079563617706299\n",
      "epoch=606 loss=5.0761027336120605\n",
      "1\n",
      "epoch=0 loss=10.570035934448242\n",
      "epoch=100 loss=7.715263843536377\n",
      "epoch=200 loss=7.0713300704956055\n",
      "epoch=300 loss=6.806365013122559\n",
      "epoch=400 loss=6.725650787353516\n",
      "epoch=500 loss=6.5749192237854\n",
      "epoch=600 loss=6.451428413391113\n",
      "epoch=653 loss=6.3946146965026855\n",
      "2\n",
      "epoch=0 loss=10.094398498535156\n",
      "epoch=100 loss=6.217579364776611\n",
      "epoch=200 loss=5.447512149810791\n",
      "epoch=300 loss=4.898906707763672\n",
      "epoch=400 loss=4.757142066955566\n",
      "epoch=500 loss=4.633988380432129\n",
      "epoch=600 loss=4.546229839324951\n",
      "epoch=700 loss=4.492352485656738\n",
      "epoch=800 loss=4.4467878341674805\n",
      "epoch=809 loss=4.436240196228027\n",
      "3\n",
      "epoch=0 loss=10.249402046203613\n",
      "epoch=100 loss=7.317298412322998\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f698851f4786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#print(TarTensor.grad_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvector_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_tensor1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdvector_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_tensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"/mnt/data/Chenpinji/cmu_dataset\"\n",
    "filename2 = '/mnt/data/Chenpinji/cmu_dataset/cmu7/wav/arctic_a0008.wav'\n",
    "for cnt in range(4, 200):\n",
    "    select = random.randint(0,9)\n",
    "    select2 = random.randint(0,500)\n",
    "    filename_1 = path + '/' + os.listdir(path)[select] + '/' + 'wav'\n",
    "    audioname = os.listdir(filename_1)[select2]\n",
    "    filename1 = filename_1 + '/'+audioname\n",
    "    wav_tensor1, sample_rate1 = torchaudio.load(filename1, normalize=False)\n",
    "    wav_tensor2, sample_rate2 = torchaudio.load(filename2, normalize=False)\n",
    "    noise_tensor = torch.normal(mean = 400., std = 50.,size = (1, len(wav_tensor1[0])))\n",
    "    wav_tensor1 = wav_tensor1[0].to(torch.float)\n",
    "    wav_tensor2 = wav_tensor2[0].to(torch.float)\n",
    "    noise_tensor = noise_tensor[0].to(torch.float)\n",
    "    #noise_tensor = noise_tensor.cuda()\n",
    "    noise_tensor.requires_grad = True\n",
    "    \n",
    "    \n",
    "    # wav_tensor1.requires_grad = True\n",
    "    # wav_tensor2.requires_grad = True\n",
    "    optimizer = torch.optim.Adam([noise_tensor],lr = 5)\n",
    "    epochs = random.randint(500,1000)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # AdvTensor = dvector_extraction(wav_tensor1) + dvector_extraction(wav_tensor2)\n",
    "        # TarTensor = dvector_extraction(noise_tensor)\n",
    "        #print(TarTensor.grad_fn)\n",
    "        loss = loss_function(dvector_extraction(wav_tensor1 + noise_tensor), dvector_extraction(wav_tensor2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"epoch={} loss={}\".format(epoch, loss))\n",
    "        if epoch == epochs - 1:\n",
    "            audio_name = '/mnt/data/Chenpinji/Generate_Adv/dvector_Gen1000/'+'dvectorGen_'+str(cnt) + '.wav'\n",
    "            temp = wav_tensor1 + noise_tensor\n",
    "            temp = temp.detach().numpy()\n",
    "            scaled = np.array(np.clip(np.round(temp),-2 ** 15, 2 ** 15 - 1), dtype = np.int16)\n",
    "            print(\"epoch={} loss={}\".format(epoch, loss))\n",
    "            wf.write(audio_name, 16000 ,scaled)\n",
    "    print(cnt)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bfad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Jun  4 2021, 14:25:59) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "be6795e88697a9790b6af751dbe32447fc9259a59c897c8a604ee8792f1ea307"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
